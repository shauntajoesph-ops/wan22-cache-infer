<!-- ## **TeaCache4HiDream-I1** -->
# TeaCache4HiDream-I1

[TeaCache](https://github.com/LiewFeng/TeaCache) can speedup [HiDream-I1](https://github.com/HiDream-ai/HiDream-I1) 2x without much visual quality degradation, in a training-free manner.  The following image shows the results generated by TeaCache-HiDream-I1-Full with various `rel_l1_thresh` values: 0 (original), 0.17 (1.5x speedup), 0.25 (1.7x speedup), 0.3 (2.0x speedup), and 0.45 (2.6x speedup).

![visualization](../assets/TeaCache4HiDream-I1.png)

## ðŸ“ˆ Inference Latency Comparisons on a Single A100

|     HiDream-I1-Full     |        TeaCache (0.17)       |    TeaCache (0.25)   |     TeaCache (0.3)    |    TeaCache (0.45)   |
|:-----------------------:|:----------------------------:|:--------------------:|:---------------------:|:--------------------:|
|         ~50 s           |        ~34 s                 |     ~29 s            |       ~25 s           |       ~19 s          |

## Installation

```shell
pip install git+https://github.com/huggingface/diffusers
pip install --upgrade transformers protobuf tiktoken tokenizers sentencepiece
```

## Usage

You can modify the `rel_l1_thresh` in line 297 to obtain your desired trade-off between latency and visul quality. For single-gpu inference, you can use the following command:

```bash
python teacache_hidream_i1.py
```

## Citation
If you find TeaCache is useful in your research or applications, please consider giving us a star ðŸŒŸ and citing it by the following BibTeX entry.

```
@article{liu2024timestep,
  title={Timestep Embedding Tells: It's Time to Cache for Video Diffusion Model},
  author={Liu, Feng and Zhang, Shiwei and Wang, Xiaofeng and Wei, Yujie and Qiu, Haonan and Zhao, Yuzhong and Zhang, Yingya and Ye, Qixiang and Wan, Fang},
  journal={arXiv preprint arXiv:2411.19108},
  year={2024}
}
```

## Acknowledgements

We would like to thank the contributors to the [HiDream-I1](https://github.com/HiDream-ai/HiDream-I1) and [Diffusers](https://github.com/huggingface/diffusers).